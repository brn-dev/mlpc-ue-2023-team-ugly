{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from lib.data_preprocessing import remove_correlated_columns, normalize_data\n",
    "from lib.ds.bird_classes import NUM_CLASSES\n",
    "from lib.ds.dataset_loading import load_all_data\n",
    "from lib.ds.dataset_splitting import split\n",
    "from lib.ds.torch_dataset import create_data_loader\n",
    "from lib.model.attention_classifier import AttentionClassifier, AttentionClassifierHyperParameters\n",
    "from lib.attention_classifier_training import train_attention_classifier_with_cv, train_attention_classifier, test_attention_classifier\n",
    "from lib.training_hyper_parameters import TrainingHyperParameters\n",
    "from lib.ds.numpy_dataset import NumpyDataset\n",
    "from lib.model.model_persistence import save_model, load_model\n",
    "from lib.random import set_random_seed\n",
    "import lib.torch_device as tdev\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tdev.PREFERRED = 'cpu'\n",
    "device = tdev.get_torch_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, labels_train, data_test, labels_test = split(*load_all_data('dataset'), seed=69421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated_columns_to_drop = array([  0,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
      "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
      "        54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n",
      "        67,  68,  69,  70,  71,  72,  73,  74,  78,  79,  80,  81,  82,\n",
      "        83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
      "        96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
      "       109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
      "       122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 156,\n",
      "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
      "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
      "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
      "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "       248, 249, 250, 251, 252, 253, 254, 255, 292, 293, 294, 295, 296,\n",
      "       297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
      "       323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336,\n",
      "       338, 339, 340, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "       392, 393, 399, 400, 462, 463, 526, 527, 528, 529, 530])\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = remove_correlated_columns(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 100, 264)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 100, 264)\n",
      "(60, 100)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "subset_indices = [\n",
    "    bird * data_train.shape[0] // 6 + sample_nr\n",
    "    for bird in range(6)\n",
    "    for sample_nr in range(10)\n",
    "]\n",
    "# data_train_subset = data_train[subset_indices, :, :]\n",
    "# labels_train_subset = labels_train[subset_indices, :]\n",
    "data_train_subset = data_train[subset_indices, :, :]\n",
    "labels_train_subset = labels_train[subset_indices, :]\n",
    "print(data_train_subset.shape)\n",
    "print(labels_train_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = AttentionClassifierHyperParameters(\n",
    "    in_features=data_train.shape[-1],\n",
    "    out_features=NUM_CLASSES,\n",
    "    \n",
    "    self_attention=True,\n",
    "    d_model=60,\n",
    "    num_heads=12,\n",
    "    stack_size=1,\n",
    "    dropout=0.1,\n",
    "    \n",
    "    in_linear_hidden_out_features=[48, 48],\n",
    "    out_linear_hidden_out_features=[64],\n",
    "    \n",
    "    linear_activation_provider=lambda: nn.LeakyReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_MULTIPLIER = 0.5\n",
    "\n",
    "training_hyper_parameters = TrainingHyperParameters(\n",
    "    batch_size=32,\n",
    "    \n",
    "    optimizer_provider=lambda model, lr: optim.Adamax(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-9\n",
    "    ),\n",
    "    \n",
    "    num_epochs=int(100 * EPOCH_MULTIPLIER),\n",
    "    lr=1e-2,\n",
    "    lr_scheduler_provider=lambda optimizer: lr_scheduler.MultiStepLR(\n",
    "        optimizer, \n",
    "        milestones=[int(m * EPOCH_MULTIPLIER) for m in [8, 20, 50, 90]],\n",
    "        gamma=0.5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "Training AttentionClassifier with 37011 parameters\n",
      "loss_weight = tensor([ 1.0000, 15.8275,  9.6494, 10.6295, 36.9184, 17.7606, 19.8364],\n",
      "       device='cuda:0')\n",
      "Training Epoch 0  : lr = 0.010000, epoch_loss = 43.228862, num_correct =  9639, num_samples = 86400, acc = 0.111562, bacc = 0.282869\n",
      "Evaluated with loss = 0.015948, acc = 0.233437, bacc = 0.574205\n",
      "Training Epoch 1  : lr = 0.010000, epoch_loss = 26.125951, num_correct = 19847, num_samples = 86400, acc = 0.229711, bacc = 0.428997\n",
      "Evaluated with loss = 0.033663, acc = 0.435937, bacc = 0.735503\n",
      "Training Epoch 2  : lr = 0.010000, epoch_loss = 16.984505, num_correct = 44419, num_samples = 86400, acc = 0.514109, bacc = 0.542834\n",
      "Evaluated with loss = 0.102852, acc = 0.548333, bacc = 0.761396\n",
      "Training Epoch 3  : lr = 0.010000, epoch_loss = 12.703956, num_correct = 52704, num_samples = 86400, acc = 0.610000, bacc = 0.613171\n",
      "Evaluated with loss = 0.076338, acc = 0.684479, bacc = 0.813050\n",
      "Training Epoch 4  : lr = 0.005000, epoch_loss = 8.766708, num_correct = 62460, num_samples = 86400, acc = 0.722917, bacc = 0.667703\n",
      "Evaluated with loss = 0.065764, acc = 0.715625, bacc = 0.807898\n",
      "Training Epoch 5  : lr = 0.005000, epoch_loss = 6.777572, num_correct = 67715, num_samples = 86400, acc = 0.783738, bacc = 0.709067\n",
      "Evaluated with loss = 0.064326, acc = 0.741875, bacc = 0.833783\n",
      "Training Epoch 6  : lr = 0.005000, epoch_loss = 5.669107, num_correct = 70186, num_samples = 86400, acc = 0.812338, bacc = 0.740713\n",
      "Evaluated with loss = 0.063701, acc = 0.793229, bacc = 0.856198\n",
      "Training Epoch 7  : lr = 0.005000, epoch_loss = 4.725964, num_correct = 72289, num_samples = 86400, acc = 0.836678, bacc = 0.765983\n",
      "Evaluated with loss = 0.082023, acc = 0.796979, bacc = 0.860620\n",
      "Training Epoch 8  : lr = 0.005000, epoch_loss = 4.377002, num_correct = 72693, num_samples = 86400, acc = 0.841354, bacc = 0.786059\n",
      "Evaluated with loss = 0.125568, acc = 0.830104, bacc = 0.872390\n",
      "Training Epoch 9  : lr = 0.005000, epoch_loss = 3.945787, num_correct = 73552, num_samples = 86400, acc = 0.851296, bacc = 0.802668\n",
      "Evaluated with loss = 0.115350, acc = 0.835000, bacc = 0.859732\n",
      "Training Epoch 10 : lr = 0.002500, epoch_loss = 3.728775, num_correct = 73969, num_samples = 86400, acc = 0.856123, bacc = 0.816500\n",
      "Evaluated with loss = 0.035604, acc = 0.819583, bacc = 0.886682\n",
      "Training Epoch 11 : lr = 0.002500, epoch_loss = 3.556883, num_correct = 75073, num_samples = 86400, acc = 0.868900, bacc = 0.828238\n",
      "Evaluated with loss = 0.062242, acc = 0.834792, bacc = 0.881501\n",
      "Training Epoch 12 : lr = 0.002500, epoch_loss = 3.238771, num_correct = 75505, num_samples = 86400, acc = 0.873900, bacc = 0.838467\n",
      "Evaluated with loss = 0.064141, acc = 0.832604, bacc = 0.876772\n",
      "Training Epoch 13 : lr = 0.002500, epoch_loss = 3.099083, num_correct = 75798, num_samples = 86400, acc = 0.877292, bacc = 0.847324\n",
      "Evaluated with loss = 0.074517, acc = 0.831771, bacc = 0.865521\n",
      "Training Epoch 14 : lr = 0.002500, epoch_loss = 2.945730, num_correct = 76153, num_samples = 86400, acc = 0.881400, bacc = 0.855216\n",
      "Evaluated with loss = 0.074451, acc = 0.836979, bacc = 0.872529\n",
      "Training Epoch 15 : lr = 0.002500, epoch_loss = 2.711253, num_correct = 76594, num_samples = 86400, acc = 0.886505, bacc = 0.862316\n",
      "Evaluated with loss = 0.060574, acc = 0.843542, bacc = 0.877294\n",
      "Training Epoch 16 : lr = 0.002500, epoch_loss = 2.577084, num_correct = 76852, num_samples = 86400, acc = 0.889491, bacc = 0.868658\n",
      "Evaluated with loss = 0.064530, acc = 0.841458, bacc = 0.873309\n",
      "Training Epoch 17 : lr = 0.002500, epoch_loss = 2.478429, num_correct = 77224, num_samples = 86400, acc = 0.893796, bacc = 0.874361\n",
      "Evaluated with loss = 0.079754, acc = 0.847083, bacc = 0.878945\n",
      "Training Epoch 18 : lr = 0.002500, epoch_loss = 2.376651, num_correct = 77557, num_samples = 86400, acc = 0.897650, bacc = 0.879546\n",
      "Evaluated with loss = 0.077781, acc = 0.851771, bacc = 0.876504\n",
      "Training Epoch 19 : lr = 0.002500, epoch_loss = 2.369369, num_correct = 77664, num_samples = 86400, acc = 0.898889, bacc = 0.884221\n",
      "Evaluated with loss = 0.089089, acc = 0.869062, bacc = 0.873943\n",
      "Training Epoch 20 : lr = 0.002500, epoch_loss = 2.482238, num_correct = 77158, num_samples = 86400, acc = 0.893032, bacc = 0.888332\n",
      "Evaluated with loss = 0.090358, acc = 0.863646, bacc = 0.874052\n",
      "Training Epoch 21 : lr = 0.002500, epoch_loss = 2.503479, num_correct = 77283, num_samples = 86400, acc = 0.894479, bacc = 0.892065\n",
      "Evaluated with loss = 0.083475, acc = 0.850208, bacc = 0.865658\n",
      "Training Epoch 22 : lr = 0.002500, epoch_loss = 2.439906, num_correct = 77666, num_samples = 86400, acc = 0.898912, bacc = 0.895512\n",
      "Evaluated with loss = 0.092131, acc = 0.855938, bacc = 0.884869\n",
      "Training Epoch 23 : lr = 0.002500, epoch_loss = 2.187236, num_correct = 78376, num_samples = 86400, acc = 0.907130, bacc = 0.898828\n",
      "Evaluated with loss = 0.094637, acc = 0.858542, bacc = 0.875195\n",
      "Training Epoch 24 : lr = 0.002500, epoch_loss = 2.066776, num_correct = 78521, num_samples = 86400, acc = 0.908808, bacc = 0.901934\n",
      "Evaluated with loss = 0.126660, acc = 0.866354, bacc = 0.870477\n",
      "Training Epoch 25 : lr = 0.001250, epoch_loss = 2.076665, num_correct = 78540, num_samples = 86400, acc = 0.909028, bacc = 0.904808\n",
      "Evaluated with loss = 0.099257, acc = 0.852292, bacc = 0.863481\n",
      "Training Epoch 26 : lr = 0.001250, epoch_loss = 2.020911, num_correct = 78688, num_samples = 86400, acc = 0.910741, bacc = 0.907488\n",
      "Evaluated with loss = 0.100035, acc = 0.867500, bacc = 0.861662\n",
      "Training Epoch 27 : lr = 0.001250, epoch_loss = 1.948194, num_correct = 78946, num_samples = 86400, acc = 0.913727, bacc = 0.910022\n",
      "Evaluated with loss = 0.104322, acc = 0.868125, bacc = 0.858559\n",
      "Training Epoch 28 : lr = 0.001250, epoch_loss = 1.929119, num_correct = 78992, num_samples = 86400, acc = 0.914259, bacc = 0.912382\n",
      "Evaluated with loss = 0.102229, acc = 0.863854, bacc = 0.859947\n",
      "Training Epoch 29 : lr = 0.001250, epoch_loss = 1.874363, num_correct = 79253, num_samples = 86400, acc = 0.917280, bacc = 0.914603\n",
      "Evaluated with loss = 0.103120, acc = 0.859271, bacc = 0.860718\n",
      "Training Epoch 30 : lr = 0.001250, epoch_loss = 1.826651, num_correct = 79386, num_samples = 86400, acc = 0.918819, bacc = 0.916708\n",
      "Evaluated with loss = 0.107960, acc = 0.860521, bacc = 0.860934\n",
      "Training Epoch 31 : lr = 0.001250, epoch_loss = 1.722654, num_correct = 79795, num_samples = 86400, acc = 0.923553, bacc = 0.918730\n",
      "Evaluated with loss = 0.118177, acc = 0.865417, bacc = 0.863727\n",
      "Training Epoch 32 : lr = 0.001250, epoch_loss = 1.654217, num_correct = 80046, num_samples = 86400, acc = 0.926458, bacc = 0.920666\n",
      "Evaluated with loss = 0.115365, acc = 0.863542, bacc = 0.866120\n",
      "Training Epoch 33 : lr = 0.001250, epoch_loss = 1.630948, num_correct = 80210, num_samples = 86400, acc = 0.928356, bacc = 0.922494\n",
      "Evaluated with loss = 0.125073, acc = 0.863646, bacc = 0.871914\n",
      "Training Epoch 34 : lr = 0.001250, epoch_loss = 1.683030, num_correct = 79952, num_samples = 86400, acc = 0.925370, bacc = 0.924188\n",
      "Evaluated with loss = 0.124791, acc = 0.875313, bacc = 0.871958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m set_random_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m cv_models_with_scalers \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_attention_classifier_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNumpyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\attention_classifier_training.py:52\u001b[0m, in \u001b[0;36mtrain_attention_classifier_with_cv\u001b[1;34m(hyper_parameters, training_hyper_parameters, dataset, device)\u001b[0m\n\u001b[0;32m     45\u001b[0m     save_model_with_scaler(\n\u001b[0;32m     46\u001b[0m         model,\n\u001b[0;32m     47\u001b[0m         normalization_scaler,\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_classifier cv\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     50\u001b[0m     models_with_scalers\u001b[38;5;241m.\u001b[39mappend((model, normalization_scaler))\n\u001b[1;32m---> 52\u001b[0m \u001b[43mtrain_with_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models_with_scalers\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\training.py:37\u001b[0m, in \u001b[0;36mtrain_with_cv\u001b[1;34m(dataset, train_func, n_folds)\u001b[0m\n\u001b[0;32m     29\u001b[0m labels_train \u001b[38;5;241m=\u001b[39m labels_folds[np\u001b[38;5;241m.\u001b[39msetdiff1d(\u001b[38;5;28mrange\u001b[39m(n_folds), fold)] \\\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, labels_folds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     32\u001b[0m data_train_normalized, data_validation_normalized, normalization_scaler \u001b[38;5;241m=\u001b[39m normalize_data(\n\u001b[0;32m     33\u001b[0m     data_train,\n\u001b[0;32m     34\u001b[0m     data_validation\n\u001b[0;32m     35\u001b[0m )\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtrain_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNumpyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNumpyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_validation_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_validation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalization_scaler\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\attention_classifier_training.py:33\u001b[0m, in \u001b[0;36mtrain_attention_classifier_with_cv.<locals>.train_func\u001b[1;34m(fold_nr, train_ds, eval_ds, normalization_scaler)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_func\u001b[39m(fold_nr: \u001b[38;5;28mint\u001b[39m, train_ds: NumpyDataset, eval_ds: NumpyDataset, normalization_scaler: StandardScaler):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_attention_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m     eval_data_loader \u001b[38;5;241m=\u001b[39m create_data_loader(eval_ds\u001b[38;5;241m.\u001b[39mdata, eval_ds\u001b[38;5;241m.\u001b[39mlabels, training_hyper_parameters\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\attention_classifier_training.py:80\u001b[0m, in \u001b[0;36mtrain_attention_classifier\u001b[1;34m(hyper_parameters, training_hyper_parameters, train_ds, eval_ds, device)\u001b[0m\n\u001b[0;32m     77\u001b[0m loss_weight \u001b[38;5;241m=\u001b[39m calculate_loss_weight(train_ds\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_weight \u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m \u001b[43m_train_attention_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_classifier\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\attention_classifier_training.py:137\u001b[0m, in \u001b[0;36m_train_attention_classifier\u001b[1;34m(model, train_data_loader, eval_data_loader, optimizer, loss_weight, num_epochs, lr_scheduler, device)\u001b[0m\n\u001b[0;32m    134\u001b[0m     all_target_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((all_target_labels, labels\u001b[38;5;241m.\u001b[39mcpu()))\n\u001b[0;32m    136\u001b[0m acc \u001b[38;5;241m=\u001b[39m num_correct \u001b[38;5;241m/\u001b[39m num_samples\n\u001b[1;32m--> 137\u001b[0m bacc \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbalanced_accuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_target_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_pred_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_lr(optimizer)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbacc \u001b[38;5;132;01m= :\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2180\u001b[0m, in \u001b[0;36mbalanced_accuracy_score\u001b[1;34m(y_true, y_pred, sample_weight, adjusted)\u001b[0m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbalanced_accuracy_score\u001b[39m(y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, adjusted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2112\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the balanced accuracy.\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m \n\u001b[0;32m   2114\u001b[0m \u001b[38;5;124;03m    The balanced accuracy in binary and multiclass classification problems to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2178\u001b[0m \u001b[38;5;124;03m    0.625\u001b[39;00m\n\u001b[0;32m   2179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2180\u001b[0m     C \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2182\u001b[0m         per_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(C) \u001b[38;5;241m/\u001b[39m C\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:322\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 322\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(labels)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\multiclass.py:114\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    111\u001b[0m     unique_ys \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mconcat([_unique_labels(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys])\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39munique_values(unique_ys)\n\u001b[1;32m--> 114\u001b[0m ys_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_unique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\multiclass.py:114\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m     unique_ys \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mconcat([_unique_labels(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys])\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39munique_values(unique_ys)\n\u001b[1;32m--> 114\u001b[0m ys_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable((i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_unique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys))\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\multiclass.py:26\u001b[0m, in \u001b[0;36m_unique_multiclass\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m     24\u001b[0m xp, is_array_api \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_array_api:\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(y)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\_array_api.py:84\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.unique_values\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "cv_models_with_scalers = train_attention_classifier_with_cv(\n",
    "    hyper_parameters, \n",
    "    training_hyper_parameters, \n",
    "    NumpyDataset(data_train, labels_train),\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AttentionClassifier with 37011 parameters\n",
      "loss_weight = tensor([ 1.0000, 15.3204,  9.3547, 10.4259, 36.4963, 17.9374, 19.6462],\n",
      "       device='cuda:0')\n",
      "Training Epoch 0  : lr = 0.010000, epoch_loss = 67.073360, num_correct = 15014, num_samples = 96000, acc = 0.156396, bacc = 0.229131\n",
      "Evaluated with loss = 0.000628, acc = 0.052417, bacc = 0.124972\n",
      "Training Epoch 1  : lr = 0.010000, epoch_loss = 92.815448, num_correct = 17032, num_samples = 96000, acc = 0.177417, bacc = 0.306088\n",
      "Evaluated with loss = 0.001123, acc = 0.342500, bacc = 0.092735\n",
      "Training Epoch 2  : lr = 0.010000, epoch_loss = 75.682951, num_correct = 16207, num_samples = 96000, acc = 0.168823, bacc = 0.259066\n",
      "Evaluated with loss = 0.000623, acc = 0.049833, bacc = 0.149810\n",
      "Training Epoch 3  : lr = 0.010000, epoch_loss = 59.135818, num_correct =  5115, num_samples = 96000, acc = 0.053281, bacc = 0.227673\n",
      "Evaluated with loss = 0.000882, acc = 0.046125, bacc = 0.180099\n",
      "Training Epoch 4  : lr = 0.005000, epoch_loss = 84.967720, num_correct =  4156, num_samples = 96000, acc = 0.043292, bacc = 0.209776\n",
      "Evaluated with loss = 0.000641, acc = 0.209667, bacc = 0.173656\n",
      "Training Epoch 5  : lr = 0.005000, epoch_loss = 72.469779, num_correct = 26303, num_samples = 96000, acc = 0.273990, bacc = 0.183960\n",
      "Evaluated with loss = 0.000575, acc = 0.299708, bacc = 0.180541\n",
      "Training Epoch 6  : lr = 0.005000, epoch_loss = 58.543011, num_correct = 19693, num_samples = 96000, acc = 0.205135, bacc = 0.175105\n",
      "Evaluated with loss = 0.000545, acc = 0.350125, bacc = 0.246749\n",
      "Training Epoch 7  : lr = 0.005000, epoch_loss = 54.710239, num_correct = 40018, num_samples = 96000, acc = 0.416854, bacc = 0.181039\n",
      "Evaluated with loss = 0.000568, acc = 0.354792, bacc = 0.332075\n",
      "Training Epoch 8  : lr = 0.005000, epoch_loss = 62.438755, num_correct = 34887, num_samples = 96000, acc = 0.363406, bacc = 0.172705\n",
      "Evaluated with loss = 0.000497, acc = 0.405708, bacc = 0.231694\n",
      "Training Epoch 9  : lr = 0.005000, epoch_loss = 54.031865, num_correct = 44563, num_samples = 96000, acc = 0.464198, bacc = 0.183849\n",
      "Evaluated with loss = 0.000480, acc = 0.408125, bacc = 0.358678\n",
      "Training Epoch 10 : lr = 0.002500, epoch_loss = 48.776596, num_correct = 51452, num_samples = 96000, acc = 0.535958, bacc = 0.203295\n",
      "Evaluated with loss = 0.000480, acc = 0.417250, bacc = 0.339858\n",
      "Training Epoch 11 : lr = 0.002500, epoch_loss = 45.640636, num_correct = 42862, num_samples = 96000, acc = 0.446479, bacc = 0.216562\n",
      "Evaluated with loss = 0.000355, acc = 0.517792, bacc = 0.395730\n",
      "Training Epoch 12 : lr = 0.002500, epoch_loss = 43.918818, num_correct = 60622, num_samples = 96000, acc = 0.631479, bacc = 0.226839\n",
      "Evaluated with loss = 0.000415, acc = 0.512250, bacc = 0.476156\n",
      "Training Epoch 13 : lr = 0.002500, epoch_loss = 40.834507, num_correct = 44888, num_samples = 96000, acc = 0.467583, bacc = 0.244911\n",
      "Evaluated with loss = 0.000421, acc = 0.534292, bacc = 0.547025\n",
      "Training Epoch 14 : lr = 0.002500, epoch_loss = 36.541874, num_correct = 58316, num_samples = 96000, acc = 0.607458, bacc = 0.258954\n",
      "Evaluated with loss = 0.000318, acc = 0.641250, bacc = 0.695688\n",
      "Training Epoch 15 : lr = 0.002500, epoch_loss = 33.772676, num_correct = 63197, num_samples = 96000, acc = 0.658302, bacc = 0.277337\n",
      "Evaluated with loss = 0.000323, acc = 0.603958, bacc = 0.613110\n",
      "Training Epoch 16 : lr = 0.002500, epoch_loss = 29.492081, num_correct = 60123, num_samples = 96000, acc = 0.626281, bacc = 0.298638\n",
      "Evaluated with loss = 0.000252, acc = 0.712333, bacc = 0.735571\n",
      "Training Epoch 17 : lr = 0.002500, epoch_loss = 27.124556, num_correct = 67431, num_samples = 96000, acc = 0.702406, bacc = 0.318567\n",
      "Evaluated with loss = 0.000283, acc = 0.663750, bacc = 0.724423\n",
      "Training Epoch 18 : lr = 0.002500, epoch_loss = 24.584294, num_correct = 70190, num_samples = 96000, acc = 0.731146, bacc = 0.338880\n",
      "Evaluated with loss = 0.000258, acc = 0.719458, bacc = 0.742235\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m set_random_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m data_train_normalized, data_test_normalized, normalization_scaler \u001b[38;5;241m=\u001b[39m normalize_data(data_train, data_test)\n\u001b[1;32m----> 5\u001b[0m attention_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_attention_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNumpyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNumpyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_test_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# test_loader = create_data_loader(np.random.normal(size=(240, 100, 268)), np.random.randint(0, 7, size=(240, 100)))\u001b[39;00m\n\u001b[0;32m     14\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m create_data_loader(data_test_normalized, labels_test)\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\attention_classifier_training.py:79\u001b[0m, in \u001b[0;36mtrain_attention_classifier\u001b[1;34m(hyper_parameters, training_hyper_parameters, train_ds, eval_ds, device)\u001b[0m\n\u001b[0;32m     76\u001b[0m loss_weight \u001b[38;5;241m=\u001b[39m calculate_loss_weight(train_ds\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_weight \u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[43m_train_attention_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_classifier\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\attention_classifier_training.py:136\u001b[0m, in \u001b[0;36m_train_attention_classifier\u001b[1;34m(model, train_data_loader, eval_data_loader, optimizer, loss_weight, num_epochs, lr_scheduler, device)\u001b[0m\n\u001b[0;32m    133\u001b[0m     all_target_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((all_target_labels, labels\u001b[38;5;241m.\u001b[39mcpu()))\n\u001b[0;32m    135\u001b[0m acc \u001b[38;5;241m=\u001b[39m num_correct \u001b[38;5;241m/\u001b[39m num_samples\n\u001b[1;32m--> 136\u001b[0m bacc \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbalanced_accuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_target_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_pred_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_lr(optimizer)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbacc \u001b[38;5;132;01m= :\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    145\u001b[0m )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2180\u001b[0m, in \u001b[0;36mbalanced_accuracy_score\u001b[1;34m(y_true, y_pred, sample_weight, adjusted)\u001b[0m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbalanced_accuracy_score\u001b[39m(y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, adjusted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2112\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the balanced accuracy.\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m \n\u001b[0;32m   2114\u001b[0m \u001b[38;5;124;03m    The balanced accuracy in binary and multiclass classification problems to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2178\u001b[0m \u001b[38;5;124;03m    0.625\u001b[39;00m\n\u001b[0;32m   2179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2180\u001b[0m     C \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2182\u001b[0m         per_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(C) \u001b[38;5;241m/\u001b[39m C\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:322\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 322\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(labels)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\multiclass.py:114\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    111\u001b[0m     unique_ys \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mconcat([_unique_labels(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys])\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39munique_values(unique_ys)\n\u001b[1;32m--> 114\u001b[0m ys_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_unique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\multiclass.py:114\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m     unique_ys \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mconcat([_unique_labels(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys])\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39munique_values(unique_ys)\n\u001b[1;32m--> 114\u001b[0m ys_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable((i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_unique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys))\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\multiclass.py:26\u001b[0m, in \u001b[0;36m_unique_multiclass\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m     24\u001b[0m xp, is_array_api \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_array_api:\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(y)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\_array_api.py:84\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.unique_values\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "data_train_normalized, data_test_normalized, normalization_scaler = normalize_data(data_train, data_test)\n",
    "\n",
    "attention_classifier = train_attention_classifier(\n",
    "    hyper_parameters, \n",
    "    training_hyper_parameters, \n",
    "    NumpyDataset(data_train_normalized, labels_train),\n",
    "    NumpyDataset(data_test_normalized, labels_test),\n",
    "    device\n",
    ")\n",
    "\n",
    "# test_loader = create_data_loader(np.random.normal(size=(240, 100, 268)), np.random.randint(0, 7, size=(240, 100)))\n",
    "test_loader = create_data_loader(data_test_normalized, labels_test)\n",
    "test_attention_classifier(\n",
    "    attention_classifier, \n",
    "    test_loader, \n",
    "    device, \n",
    "    show_confmat = True,\n",
    "    confmat_title = 'Test Set Performance'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_models\\\\attention_classifier bacc-0.856688.pt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_model(attention_classifier, 'attention_classifier bacc-0.856688')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_test_normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load_model('attention_classifier fold-1')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# TODO: normalization\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m create_data_loader(\u001b[43mdata_test_normalized\u001b[49m, labels_test)\n\u001b[0;32m      4\u001b[0m test_attention_classifier(load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv1/attention_classifier fold-0\u001b[39m\u001b[38;5;124m'\u001b[39m), test_loader, device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_test_normalized' is not defined"
     ]
    }
   ],
   "source": [
    "# load_model('attention_classifier fold-1')\n",
    "# TODO: normalization\n",
    "test_loader = create_data_loader(data_test_normalized, labels_test)\n",
    "test_attention_classifier(load_model('cv1/attention_classifier fold-0'), test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
