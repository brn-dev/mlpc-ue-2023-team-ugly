{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from lib.data_preprocessing import remove_correlated_columns\n",
    "from lib.ds.bird_classes import NUM_CLASSES\n",
    "from lib.ds.dataset_loading import load_all_data\n",
    "from lib.ds.dataset_splitting import split\n",
    "from lib.ds.torch_dataset import create_offset_data_loader\n",
    "from lib.model.classification_transformer import TransformerHyperParameters\n",
    "from lib.transformer_training import train_transformer_with_cv, train_transformer, test_network\n",
    "from lib.training_hyper_parameters import TrainingHyperParameters\n",
    "from lib.random import set_random_seed\n",
    "import lib.torch_device as tdev\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tdev.PREFERRED = 'cpu'\n",
    "device = tdev.get_torch_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, labels_train, data_test, labels_test = split(*load_all_data('dataset'), seed=69420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = remove_correlated_columns(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 100, 268)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 100, 268)\n",
      "(60, 100)\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "subset_indices = [\n",
    "    bird * data_train.shape[0] // 6 + sample_nr\n",
    "    for bird in range(6)\n",
    "    for sample_nr in range(10)\n",
    "]\n",
    "# data_train_subset = data_train[subset_indices, :, :]\n",
    "# labels_train_subset = labels_train[subset_indices, :]\n",
    "data_train_subset = data_train[subset_indices, :, :]\n",
    "labels_train_subset = labels_train[subset_indices, :]\n",
    "print(data_train_subset.shape)\n",
    "print(labels_train_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default\n",
    "hyper_parameters = TransformerHyperParameters(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    out_size=NUM_CLASSES,\n",
    "    in_features=data_train.shape[-1],\n",
    ")\n",
    "## Current\n",
    "hyper_parameters = TransformerHyperParameters(\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1,\n",
    "    out_size=NUM_CLASSES,\n",
    "    in_features=data_train.shape[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hyper_parameters = TrainingHyperParameters(\n",
    "    batch_size=128,\n",
    "    num_epochs=150,\n",
    "    lr=1e-2,\n",
    "    lr_scheduler_provider=lambda optimizer: lr_scheduler.MultiStepLR(\n",
    "        optimizer, \n",
    "        milestones=[10, 30, 60, 100], \n",
    "        gamma=0.1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.min(labels) = 0.0\n",
      "torch.flatten(torch.Tensor(labels).long()).min() = tensor(0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.04 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m set_random_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_transformer_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\transformer_training.py:32\u001b[0m, in \u001b[0;36mtrain_transformer_with_cv\u001b[1;34m(data, labels, hyper_parameters, training_hyper_parameters, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m create_offset_data_loader(d, l, batch_size\u001b[38;5;241m=\u001b[39mtraining_hyper_parameters\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     29\u001b[0m     test_network(network, data_loader, device)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mtrain_with_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_and_train_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\training.py:36\u001b[0m, in \u001b[0;36mtrain_with_cv\u001b[1;34m(data, labels, create_and_train_func, eval_func, n_folds)\u001b[0m\n\u001b[0;32m     31\u001b[0m labels_train \u001b[38;5;241m=\u001b[39m labels_folds[np\u001b[38;5;241m.\u001b[39msetdiff1d(\u001b[38;5;28mrange\u001b[39m(n_folds), fold)] \\\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, labels_folds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     34\u001b[0m data_train_normalized, data_validation_normalized \u001b[38;5;241m=\u001b[39m normalize_data(data_train, data_validation)\n\u001b[1;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_train_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m eval_func(model, data_validation_normalized, labels_validation)\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\transformer_training.py:25\u001b[0m, in \u001b[0;36mtrain_transformer_with_cv.<locals>.create_and_train_func\u001b[1;34m(d, l)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_and_train_func\u001b[39m(d: np\u001b[38;5;241m.\u001b[39mndarray, l: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\transformer_training.py:52\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[1;34m(data, labels, hyper_parameters, training_hyper_parameters, device)\u001b[0m\n\u001b[0;32m     48\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m create_offset_data_loader(data, labels, training_hyper_parameters\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     50\u001b[0m loss_weight \u001b[38;5;241m=\u001b[39m calculate_loss_weight(labels)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformer\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\transformer_training.py:86\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, data_loader, optimizer, loss_weight, num_epochs, lr_scheduler, device)\u001b[0m\n\u001b[0;32m     81\u001b[0m data, target_input, target_expected \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m     82\u001b[0m     data\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), target_input\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device), target_expected\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 86\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m pred, target_expected \u001b[38;5;241m=\u001b[39m reshape_for_loss(pred, target_expected)\n\u001b[0;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target_expected)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\model\\classification_transformer.py:74\u001b[0m, in \u001b[0;36mClassificationTransformer.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     71\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(get_torch_device())\n\u001b[0;32m     72\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(get_torch_device())\n\u001b[1;32m---> 74\u001b[0m transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_fnn(transformer_out)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:142\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    141\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask)\n\u001b[1;32m--> 142\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:248\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    245\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 248\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:452\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[1;32m--> 452\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    453\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:473\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[1;34m(self, x, mem, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[0;32m    468\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    469\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn(x, mem, mem,\n\u001b[0;32m    470\u001b[0m                             attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    471\u001b[0m                             key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m    472\u001b[0m                             need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\functional.py:1169\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.04 GiB already allocated; 0 bytes free; 7.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "train_transformer_with_cv(\n",
    "    data_train_subset, \n",
    "    labels_train_subset, \n",
    "    hyper_parameters, \n",
    "    training_hyper_parameters, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.min(labels) = 0.0\n",
      "torch.flatten(torch.Tensor(labels).long()).min() = tensor(0)\n",
      "loss_weight = tensor([ 1.0000,  9.9554,  6.6479,  8.8590, 38.6731, 20.2111, 18.9717],\n",
      "       device='cuda:0')\n",
      "Epoch  0: epoch_loss = 2.262574, num_correct =  3512, num_samples =  6000, acc = 0.585333, bacc = 0.126696\n",
      "Epoch  1: epoch_loss = 2.969042, num_correct =   104, num_samples =  6000, acc = 0.017333, bacc = 0.134777\n",
      "Epoch  2: epoch_loss = 3.069208, num_correct =   347, num_samples =  6000, acc = 0.057833, bacc = 0.136290\n",
      "Epoch  3: epoch_loss = 2.635720, num_correct =   217, num_samples =  6000, acc = 0.036167, bacc = 0.137319\n",
      "Epoch  4: epoch_loss = 2.376815, num_correct =   600, num_samples =  6000, acc = 0.100000, bacc = 0.138365\n",
      "Epoch  5: epoch_loss = 2.110154, num_correct =   577, num_samples =  6000, acc = 0.096167, bacc = 0.138940\n",
      "Epoch  6: epoch_loss = 2.001911, num_correct =   107, num_samples =  6000, acc = 0.017833, bacc = 0.139686\n",
      "Epoch  7: epoch_loss = 2.068061, num_correct =  3294, num_samples =  6000, acc = 0.549000, bacc = 0.139861\n",
      "Epoch  8: epoch_loss = 2.191526, num_correct =  4022, num_samples =  6000, acc = 0.670333, bacc = 0.140194\n",
      "Epoch  9: epoch_loss = 2.172211, num_correct =  4022, num_samples =  6000, acc = 0.670333, bacc = 0.140460\n",
      "Epoch 10: epoch_loss = 2.066144, num_correct =  4017, num_samples =  6000, acc = 0.669500, bacc = 0.140687\n",
      "Epoch 11: epoch_loss = 2.056606, num_correct =  4014, num_samples =  6000, acc = 0.669000, bacc = 0.140891\n",
      "Epoch 12: epoch_loss = 2.043449, num_correct =  3986, num_samples =  6000, acc = 0.664333, bacc = 0.140987\n",
      "Epoch 13: epoch_loss = 2.036158, num_correct =  3962, num_samples =  6000, acc = 0.660333, bacc = 0.141151\n",
      "Epoch 14: epoch_loss = 2.023229, num_correct =  3874, num_samples =  6000, acc = 0.645667, bacc = 0.141117\n",
      "Epoch 15: epoch_loss = 2.006067, num_correct =  3734, num_samples =  6000, acc = 0.622333, bacc = 0.141138\n",
      "Epoch 16: epoch_loss = 1.996736, num_correct =  3498, num_samples =  6000, acc = 0.583000, bacc = 0.141202\n",
      "Epoch 17: epoch_loss = 1.987002, num_correct =  3095, num_samples =  6000, acc = 0.515833, bacc = 0.140895\n",
      "Epoch 18: epoch_loss = 1.977110, num_correct =  2612, num_samples =  6000, acc = 0.435333, bacc = 0.141025\n",
      "Epoch 19: epoch_loss = 1.965523, num_correct =  2045, num_samples =  6000, acc = 0.340833, bacc = 0.141253\n",
      "Epoch 20: epoch_loss = 1.962819, num_correct =  1375, num_samples =  6000, acc = 0.229167, bacc = 0.141750\n",
      "Epoch 21: epoch_loss = 1.960844, num_correct =   861, num_samples =  6000, acc = 0.143500, bacc = 0.141441\n",
      "Epoch 22: epoch_loss = 1.952230, num_correct =   559, num_samples =  6000, acc = 0.093167, bacc = 0.141373\n",
      "Epoch 23: epoch_loss = 1.955536, num_correct =   417, num_samples =  6000, acc = 0.069500, bacc = 0.141278\n",
      "Epoch 24: epoch_loss = 1.960816, num_correct =   368, num_samples =  6000, acc = 0.061333, bacc = 0.141106\n",
      "Epoch 25: epoch_loss = 1.958768, num_correct =   388, num_samples =  6000, acc = 0.064667, bacc = 0.140787\n",
      "Epoch 26: epoch_loss = 1.960443, num_correct =   428, num_samples =  6000, acc = 0.071333, bacc = 0.140903\n",
      "Epoch 27: epoch_loss = 1.963217, num_correct =   474, num_samples =  6000, acc = 0.079000, bacc = 0.141114\n",
      "Epoch 28: epoch_loss = 1.962862, num_correct =   487, num_samples =  6000, acc = 0.081167, bacc = 0.141378\n",
      "Epoch 29: epoch_loss = 1.961488, num_correct =   481, num_samples =  6000, acc = 0.080167, bacc = 0.141281\n",
      "Epoch 30: epoch_loss = 1.968639, num_correct =   464, num_samples =  6000, acc = 0.077333, bacc = 0.140967\n",
      "Epoch 31: epoch_loss = 1.966199, num_correct =   493, num_samples =  6000, acc = 0.082167, bacc = 0.141097\n",
      "Epoch 32: epoch_loss = 1.964236, num_correct =   504, num_samples =  6000, acc = 0.084000, bacc = 0.141380\n",
      "Epoch 33: epoch_loss = 1.962118, num_correct =   484, num_samples =  6000, acc = 0.080667, bacc = 0.141435\n",
      "Epoch 34: epoch_loss = 1.966448, num_correct =   487, num_samples =  6000, acc = 0.081167, bacc = 0.141528\n",
      "Epoch 35: epoch_loss = 1.960205, num_correct =   503, num_samples =  6000, acc = 0.083833, bacc = 0.141943\n",
      "Epoch 36: epoch_loss = 1.963210, num_correct =   505, num_samples =  6000, acc = 0.084167, bacc = 0.142132\n",
      "Epoch 37: epoch_loss = 1.960795, num_correct =   474, num_samples =  6000, acc = 0.079000, bacc = 0.142088\n",
      "Epoch 38: epoch_loss = 1.966759, num_correct =   479, num_samples =  6000, acc = 0.079833, bacc = 0.142203\n",
      "Epoch 39: epoch_loss = 1.969316, num_correct =   451, num_samples =  6000, acc = 0.075167, bacc = 0.141937\n",
      "Epoch 40: epoch_loss = 1.961472, num_correct =   480, num_samples =  6000, acc = 0.080000, bacc = 0.142125\n",
      "Epoch 41: epoch_loss = 1.963102, num_correct =   463, num_samples =  6000, acc = 0.077167, bacc = 0.142180\n",
      "Epoch 42: epoch_loss = 1.958861, num_correct =   492, num_samples =  6000, acc = 0.082000, bacc = 0.142527\n",
      "Epoch 43: epoch_loss = 1.963304, num_correct =   459, num_samples =  6000, acc = 0.076500, bacc = 0.142701\n",
      "Epoch 44: epoch_loss = 1.963918, num_correct =   458, num_samples =  6000, acc = 0.076333, bacc = 0.142646\n",
      "Epoch 45: epoch_loss = 1.964567, num_correct =   426, num_samples =  6000, acc = 0.071000, bacc = 0.142500\n",
      "Epoch 46: epoch_loss = 1.960744, num_correct =   454, num_samples =  6000, acc = 0.075667, bacc = 0.142736\n",
      "Epoch 47: epoch_loss = 1.960229, num_correct =   455, num_samples =  6000, acc = 0.075833, bacc = 0.142864\n",
      "Epoch 48: epoch_loss = 1.960998, num_correct =   444, num_samples =  6000, acc = 0.074000, bacc = 0.142918\n",
      "Epoch 49: epoch_loss = 1.960024, num_correct =   432, num_samples =  6000, acc = 0.072000, bacc = 0.142941\n",
      "Epoch 50: epoch_loss = 1.956874, num_correct =   436, num_samples =  6000, acc = 0.072667, bacc = 0.142943\n",
      "Epoch 51: epoch_loss = 1.958870, num_correct =   413, num_samples =  6000, acc = 0.068833, bacc = 0.142952\n",
      "Epoch 52: epoch_loss = 1.963240, num_correct =   410, num_samples =  6000, acc = 0.068333, bacc = 0.142820\n",
      "Epoch 53: epoch_loss = 1.957277, num_correct =   424, num_samples =  6000, acc = 0.070667, bacc = 0.143077\n",
      "Epoch 54: epoch_loss = 1.953288, num_correct =   411, num_samples =  6000, acc = 0.068500, bacc = 0.143002\n",
      "Epoch 55: epoch_loss = 1.958852, num_correct =   393, num_samples =  6000, acc = 0.065500, bacc = 0.142991\n",
      "Epoch 56: epoch_loss = 1.959599, num_correct =   388, num_samples =  6000, acc = 0.064667, bacc = 0.142797\n",
      "Epoch 57: epoch_loss = 1.960073, num_correct =   384, num_samples =  6000, acc = 0.064000, bacc = 0.142728\n",
      "Epoch 58: epoch_loss = 1.955037, num_correct =   384, num_samples =  6000, acc = 0.064000, bacc = 0.142672\n",
      "Epoch 59: epoch_loss = 1.960076, num_correct =   367, num_samples =  6000, acc = 0.061167, bacc = 0.142416\n",
      "Epoch 60: epoch_loss = 1.956484, num_correct =   380, num_samples =  6000, acc = 0.063333, bacc = 0.142359\n",
      "Epoch 61: epoch_loss = 1.959975, num_correct =   380, num_samples =  6000, acc = 0.063333, bacc = 0.142258\n",
      "Epoch 62: epoch_loss = 1.958118, num_correct =   358, num_samples =  6000, acc = 0.059667, bacc = 0.142079\n",
      "Epoch 63: epoch_loss = 1.957469, num_correct =   350, num_samples =  6000, acc = 0.058333, bacc = 0.141952\n",
      "Epoch 64: epoch_loss = 1.957470, num_correct =   395, num_samples =  6000, acc = 0.065833, bacc = 0.142091\n",
      "Epoch 65: epoch_loss = 1.953463, num_correct =   396, num_samples =  6000, acc = 0.066000, bacc = 0.142268\n",
      "Epoch 66: epoch_loss = 1.960632, num_correct =   370, num_samples =  6000, acc = 0.061667, bacc = 0.142129\n",
      "Epoch 67: epoch_loss = 1.955032, num_correct =   355, num_samples =  6000, acc = 0.059167, bacc = 0.142057\n",
      "Epoch 68: epoch_loss = 1.957459, num_correct =   369, num_samples =  6000, acc = 0.061500, bacc = 0.142047\n",
      "Epoch 69: epoch_loss = 1.961257, num_correct =   364, num_samples =  6000, acc = 0.060667, bacc = 0.141883\n",
      "Epoch 70: epoch_loss = 1.957663, num_correct =   366, num_samples =  6000, acc = 0.061000, bacc = 0.141813\n",
      "Epoch 71: epoch_loss = 1.957365, num_correct =   381, num_samples =  6000, acc = 0.063500, bacc = 0.141940\n",
      "Epoch 72: epoch_loss = 1.958359, num_correct =   382, num_samples =  6000, acc = 0.063667, bacc = 0.141955\n",
      "Epoch 73: epoch_loss = 1.959725, num_correct =   356, num_samples =  6000, acc = 0.059333, bacc = 0.141840\n",
      "Epoch 74: epoch_loss = 1.955556, num_correct =   387, num_samples =  6000, acc = 0.064500, bacc = 0.141976\n",
      "Epoch 75: epoch_loss = 1.955693, num_correct =   392, num_samples =  6000, acc = 0.065333, bacc = 0.142046\n",
      "Epoch 76: epoch_loss = 1.956202, num_correct =   388, num_samples =  6000, acc = 0.064667, bacc = 0.142063\n",
      "Epoch 77: epoch_loss = 1.953148, num_correct =   404, num_samples =  6000, acc = 0.067333, bacc = 0.142193\n",
      "Epoch 78: epoch_loss = 1.957206, num_correct =   362, num_samples =  6000, acc = 0.060333, bacc = 0.142143\n",
      "Epoch 79: epoch_loss = 1.958861, num_correct =   392, num_samples =  6000, acc = 0.065333, bacc = 0.142192\n",
      "Epoch 80: epoch_loss = 1.957594, num_correct =   379, num_samples =  6000, acc = 0.063167, bacc = 0.142223\n",
      "Epoch 81: epoch_loss = 1.955506, num_correct =   404, num_samples =  6000, acc = 0.067333, bacc = 0.142278\n",
      "Epoch 82: epoch_loss = 1.966653, num_correct =   338, num_samples =  6000, acc = 0.056333, bacc = 0.142060\n",
      "Epoch 83: epoch_loss = 1.954541, num_correct =   368, num_samples =  6000, acc = 0.061333, bacc = 0.141995\n",
      "Epoch 84: epoch_loss = 1.958114, num_correct =   364, num_samples =  6000, acc = 0.060667, bacc = 0.141937\n",
      "Epoch 85: epoch_loss = 1.954770, num_correct =   385, num_samples =  6000, acc = 0.064167, bacc = 0.142044\n",
      "Epoch 86: epoch_loss = 1.958391, num_correct =   403, num_samples =  6000, acc = 0.067167, bacc = 0.142128\n",
      "Epoch 87: epoch_loss = 1.955287, num_correct =   363, num_samples =  6000, acc = 0.060500, bacc = 0.142091\n",
      "Epoch 88: epoch_loss = 1.952680, num_correct =   372, num_samples =  6000, acc = 0.062000, bacc = 0.142080\n",
      "Epoch 89: epoch_loss = 1.953671, num_correct =   402, num_samples =  6000, acc = 0.067000, bacc = 0.142172\n",
      "Epoch 90: epoch_loss = 1.954070, num_correct =   376, num_samples =  6000, acc = 0.062667, bacc = 0.142161\n",
      "Epoch 91: epoch_loss = 1.958206, num_correct =   370, num_samples =  6000, acc = 0.061667, bacc = 0.142099\n",
      "Epoch 92: epoch_loss = 1.956489, num_correct =   381, num_samples =  6000, acc = 0.063500, bacc = 0.142096\n",
      "Epoch 93: epoch_loss = 1.959153, num_correct =   374, num_samples =  6000, acc = 0.062333, bacc = 0.142173\n",
      "Epoch 94: epoch_loss = 1.954462, num_correct =   391, num_samples =  6000, acc = 0.065167, bacc = 0.142247\n",
      "Epoch 95: epoch_loss = 1.952709, num_correct =   399, num_samples =  6000, acc = 0.066500, bacc = 0.142377\n",
      "Epoch 96: epoch_loss = 1.952263, num_correct =   410, num_samples =  6000, acc = 0.068333, bacc = 0.142476\n",
      "Epoch 97: epoch_loss = 1.959572, num_correct =   353, num_samples =  6000, acc = 0.058833, bacc = 0.142426\n",
      "Epoch 98: epoch_loss = 1.961478, num_correct =   380, num_samples =  6000, acc = 0.063333, bacc = 0.142401\n",
      "Epoch 99: epoch_loss = 1.955958, num_correct =   392, num_samples =  6000, acc = 0.065333, bacc = 0.142430\n",
      "Epoch 100: epoch_loss = 1.954722, num_correct =   368, num_samples =  6000, acc = 0.061333, bacc = 0.142456\n",
      "Epoch 101: epoch_loss = 1.958889, num_correct =   390, num_samples =  6000, acc = 0.065000, bacc = 0.142432\n",
      "Epoch 102: epoch_loss = 1.961961, num_correct =   362, num_samples =  6000, acc = 0.060333, bacc = 0.142355\n",
      "Epoch 103: epoch_loss = 1.956508, num_correct =   391, num_samples =  6000, acc = 0.065167, bacc = 0.142403\n",
      "Epoch 104: epoch_loss = 1.959228, num_correct =   361, num_samples =  6000, acc = 0.060167, bacc = 0.142293\n",
      "Epoch 105: epoch_loss = 1.952614, num_correct =   380, num_samples =  6000, acc = 0.063333, bacc = 0.142333\n",
      "Epoch 106: epoch_loss = 1.958307, num_correct =   349, num_samples =  6000, acc = 0.058167, bacc = 0.142256\n",
      "Epoch 107: epoch_loss = 1.958142, num_correct =   385, num_samples =  6000, acc = 0.064167, bacc = 0.142273\n",
      "Epoch 108: epoch_loss = 1.958672, num_correct =   376, num_samples =  6000, acc = 0.062667, bacc = 0.142270\n",
      "Epoch 109: epoch_loss = 1.956696, num_correct =   375, num_samples =  6000, acc = 0.062500, bacc = 0.142307\n",
      "Epoch 110: epoch_loss = 1.951228, num_correct =   399, num_samples =  6000, acc = 0.066500, bacc = 0.142370\n",
      "Epoch 111: epoch_loss = 1.960482, num_correct =   352, num_samples =  6000, acc = 0.058667, bacc = 0.142224\n",
      "Epoch 112: epoch_loss = 1.957922, num_correct =   395, num_samples =  6000, acc = 0.065833, bacc = 0.142286\n",
      "Epoch 113: epoch_loss = 1.958851, num_correct =   343, num_samples =  6000, acc = 0.057167, bacc = 0.142186\n",
      "Epoch 114: epoch_loss = 1.961578, num_correct =   336, num_samples =  6000, acc = 0.056000, bacc = 0.141981\n",
      "Epoch 115: epoch_loss = 1.957668, num_correct =   368, num_samples =  6000, acc = 0.061333, bacc = 0.141939\n",
      "Epoch 116: epoch_loss = 1.956483, num_correct =   406, num_samples =  6000, acc = 0.067667, bacc = 0.141967\n",
      "Epoch 117: epoch_loss = 1.954675, num_correct =   386, num_samples =  6000, acc = 0.064333, bacc = 0.141997\n",
      "Epoch 118: epoch_loss = 1.962154, num_correct =   374, num_samples =  6000, acc = 0.062333, bacc = 0.141967\n",
      "Epoch 119: epoch_loss = 1.955160, num_correct =   370, num_samples =  6000, acc = 0.061667, bacc = 0.141934\n",
      "Epoch 120: epoch_loss = 1.956712, num_correct =   346, num_samples =  6000, acc = 0.057667, bacc = 0.141815\n",
      "Epoch 121: epoch_loss = 1.954025, num_correct =   370, num_samples =  6000, acc = 0.061667, bacc = 0.141892\n",
      "Epoch 122: epoch_loss = 1.953816, num_correct =   359, num_samples =  6000, acc = 0.059833, bacc = 0.141843\n",
      "Epoch 123: epoch_loss = 1.951659, num_correct =   401, num_samples =  6000, acc = 0.066833, bacc = 0.141930\n",
      "Epoch 124: epoch_loss = 1.957276, num_correct =   375, num_samples =  6000, acc = 0.062500, bacc = 0.141933\n",
      "Epoch 125: epoch_loss = 1.954510, num_correct =   402, num_samples =  6000, acc = 0.067000, bacc = 0.141965\n",
      "Epoch 126: epoch_loss = 1.958067, num_correct =   389, num_samples =  6000, acc = 0.064833, bacc = 0.142013\n",
      "Epoch 127: epoch_loss = 1.954407, num_correct =   374, num_samples =  6000, acc = 0.062333, bacc = 0.141979\n",
      "Epoch 128: epoch_loss = 1.954262, num_correct =   379, num_samples =  6000, acc = 0.063167, bacc = 0.142000\n",
      "Epoch 129: epoch_loss = 1.957399, num_correct =   371, num_samples =  6000, acc = 0.061833, bacc = 0.141969\n",
      "Epoch 130: epoch_loss = 1.955824, num_correct =   390, num_samples =  6000, acc = 0.065000, bacc = 0.141923\n",
      "Epoch 131: epoch_loss = 1.954661, num_correct =   385, num_samples =  6000, acc = 0.064167, bacc = 0.141946\n",
      "Epoch 132: epoch_loss = 1.957904, num_correct =   385, num_samples =  6000, acc = 0.064167, bacc = 0.142021\n",
      "Epoch 133: epoch_loss = 1.959412, num_correct =   328, num_samples =  6000, acc = 0.054667, bacc = 0.141848\n",
      "Epoch 134: epoch_loss = 1.957857, num_correct =   342, num_samples =  6000, acc = 0.057000, bacc = 0.141753\n",
      "Epoch 135: epoch_loss = 1.955241, num_correct =   379, num_samples =  6000, acc = 0.063167, bacc = 0.141727\n",
      "Epoch 136: epoch_loss = 1.955069, num_correct =   373, num_samples =  6000, acc = 0.062167, bacc = 0.141799\n",
      "Epoch 137: epoch_loss = 1.956305, num_correct =   421, num_samples =  6000, acc = 0.070167, bacc = 0.141912\n",
      "Epoch 138: epoch_loss = 1.957169, num_correct =   349, num_samples =  6000, acc = 0.058167, bacc = 0.141837\n",
      "Epoch 139: epoch_loss = 1.955273, num_correct =   352, num_samples =  6000, acc = 0.058667, bacc = 0.141789\n",
      "Epoch 140: epoch_loss = 1.958951, num_correct =   351, num_samples =  6000, acc = 0.058500, bacc = 0.141687\n",
      "Epoch 141: epoch_loss = 1.953562, num_correct =   397, num_samples =  6000, acc = 0.066167, bacc = 0.141766\n",
      "Epoch 142: epoch_loss = 1.959404, num_correct =   378, num_samples =  6000, acc = 0.063000, bacc = 0.141692\n",
      "Epoch 143: epoch_loss = 1.954398, num_correct =   404, num_samples =  6000, acc = 0.067333, bacc = 0.141728\n",
      "Epoch 144: epoch_loss = 1.953808, num_correct =   373, num_samples =  6000, acc = 0.062167, bacc = 0.141736\n",
      "Epoch 145: epoch_loss = 1.957568, num_correct =   384, num_samples =  6000, acc = 0.064000, bacc = 0.141735\n",
      "Epoch 146: epoch_loss = 1.959148, num_correct =   374, num_samples =  6000, acc = 0.062333, bacc = 0.141680\n",
      "Epoch 147: epoch_loss = 1.955894, num_correct =   407, num_samples =  6000, acc = 0.067833, bacc = 0.141756\n",
      "Epoch 148: epoch_loss = 1.954008, num_correct =   416, num_samples =  6000, acc = 0.069333, bacc = 0.141851\n",
      "Epoch 149: epoch_loss = 1.957057, num_correct =   370, num_samples =  6000, acc = 0.061667, bacc = 0.141789\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "transformer = train_transformer(\n",
    "    data_train_subset, \n",
    "    labels_train_subset, \n",
    "    hyper_parameters, \n",
    "    training_hyper_parameters, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 1, 1, 0], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 0, 0, 3], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 0, 4, 0], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 5, 0, 5], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2], device='cuda:0') tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Evaluated with loss = 0.0006819420357545217, acc = 0.07595833333333334, bacc = 0.14285714285714285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_loader = create_offset_data_loader(np.random.normal(size=(240, 100, 268)), np.random.randint(0, 7, size=(240, 100)))\n",
    "test_loader = create_offset_data_loader(data_test, labels_test)\n",
    "test_network(transformer, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
