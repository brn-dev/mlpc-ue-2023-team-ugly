{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.decomposition\n",
    "\n",
    "from lib.data_preprocessing import remove_correlated_columns, normalize_data\n",
    "from lib.ds.bird_classes import NUM_CLASSES\n",
    "from lib.ds.dataset_loading import load_all_data, flatten\n",
    "from lib.ds.dataset_splitting import split\n",
    "from lib.ds.torch_dataset import create_data_loader\n",
    "from lib.ds.challenge_dataset import load_challenge_data\n",
    "from lib.model.attention_classifier import AttentionClassifier, AttentionClassifierHyperParameters\n",
    "from lib.model.sliding_attention_classifier import SlidingAttentionClassifier, SlidingAttentionClassifierHyperParameters\n",
    "from lib.torch_generic_model_training import train_model_with_cv\n",
    "from lib.training_hyper_parameters import TrainingHyperParameters\n",
    "from lib.ds.numpy_dataset import NumpyDataset\n",
    "from lib.model.model_persistence import save_model, load_model\n",
    "from lib.random import set_random_seed\n",
    "from lib.metrics import calculate_average_metrics_for_final_epoch_of_folds, calculate_average_metrics_per_epoch, calculate_average_metrics\n",
    "from lib.ds.bird_combiner import combine_birds\n",
    "from lib.challenge import predict_for_challenge, save_results_to_csv, load_results_from_csv\n",
    "from lib.label_fixing import fix_labels_information_gain\n",
    "import lib.torch_device as tdev\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tdev.PREFERRED = 'cpu'\n",
    "device = tdev.get_torch_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_raw.shape = (1200, 100, 548)\n",
      "labels.shape   = (1200, 100)\n"
     ]
    }
   ],
   "source": [
    "data_raw, labels = load_all_data('dataset')\n",
    "print(f'{data_raw.shape = }')\n",
    "print(f'{labels.shape   = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_raw_train.data.shape = (960, 100, 548)\n",
      "dataset_raw_test.data.shape  = (240, 100, 548)\n"
     ]
    }
   ],
   "source": [
    "dataset_raw_train, dataset_raw_test = split(NumpyDataset(data_raw, labels), test_size_pct=0.2, seed=6942066)\n",
    "\n",
    "\n",
    "print(f'{dataset_raw_train.data.shape = }')\n",
    "print(f'{dataset_raw_test.data.shape  = }' if dataset_raw_test is not None else 'dataset_raw_test             = None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating random sequence (num_duplicates = 1): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 96000/96000 [01:21<00:00, 1183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences_ds_train.data.shape   = (320, 300, 548)\n",
      "sequences_ds_train.labels.shape = (320, 300)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating random sequence (num_duplicates = 1): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 24000/24000 [00:05<00:00, 4567.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences_ds_test.data.shape   = (80, 300, 548)\n",
      "sequences_ds_test.labels.shape = (80, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sequences_ds_train = combine_birds(\n",
    "    dataset_raw_train, \n",
    "    sequence_length=300,\n",
    "    random_seed=42\n",
    ")\n",
    "print(f'{sequences_ds_train.data.shape   = }')\n",
    "print(f'{sequences_ds_train.labels.shape = }\\n\\n')\n",
    "\n",
    "\n",
    "sequences_ds_test = combine_birds(\n",
    "    dataset_raw_test, \n",
    "    sequence_length=300,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "if sequences_ds_test is not None:\n",
    "    print(f'{sequences_ds_test.data.shape   = }')\n",
    "    print(f'{sequences_ds_test.labels.shape = }')\n",
    "else:\n",
    "    print('sequences_ds_test = None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_hyper_parameters = SlidingAttentionClassifierHyperParameters(\n",
    "    in_features=data_raw.shape[-1],\n",
    "    out_features=NUM_CLASSES,\n",
    "    \n",
    "    attention_window_size=25,\n",
    "    stride=16,\n",
    "    \n",
    "    d_model=64,\n",
    "    num_heads=16,\n",
    "    attention_stack_size=2,\n",
    "    attention_stack_activation_provider=lambda: nn.LeakyReLU(),\n",
    "    attention_dropout=0.45,\n",
    "    \n",
    "    in_linear_hidden_out_features=[128, 64, 64],\n",
    "    out_linear_hidden_out_features=[32, 16],\n",
    "    linear_activation_provider=lambda: nn.LeakyReLU(),\n",
    "    linear_dropout=0.45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hyper_parameters = TrainingHyperParameters(\n",
    "        batch_size=16,\n",
    "        \n",
    "        loss_weight_factors=torch.Tensor([\n",
    "            1.0 / 3, # 0\n",
    "            1.0, # 1\n",
    "            1.0 / 0.5, # 2\n",
    "            1.0 / 0.5, # 3\n",
    "            1.0, # 4\n",
    "            1.0, # 5\n",
    "            1.0, # 6\n",
    "        ]).to(device),\n",
    "        \n",
    "        optimizer_provider=lambda model, lr: optim.Adamax(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-9\n",
    "        ),\n",
    "\n",
    "        num_epochs=200,\n",
    "        lr=1e-3,\n",
    "    \n",
    "        lr_scheduler_milestones=[int(m) for m in [30, 100, 250]],\n",
    "        lr_scheduler_gamma=0.75,\n",
    "        lr_scheduler_provider=lambda optimizer, milestones, gamma: None,\n",
    "        # lr_scheduler_provider=lambda optimizer, milestones, gamma: lr_scheduler.MultiStepLR(\n",
    "        #     optimizer, \n",
    "        #     milestones=milestones,\n",
    "        #     gamma=gamma\n",
    "        # )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 8 folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds:   0%|                                                                                                                                                                | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Executing CV for fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds:   0%|                                                                                                                                                                | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "torch.Size([1024, 1, 64])\n",
      "\n",
      "\n",
      "#### Training ####\n",
      "##################\n",
      "SlidingAttentionClassifier with 122855 parameters, in_fnn: 86848, attention_stack: 33280, out_fnn: 2727\n",
      "##################\n",
      "\n",
      "train label counts = [59904,  4134,  6065,  5655,  1709,  3788,  2745]\n",
      "eval label counts  = [ 8462,   480,  1060,   909,   219,   373,   497]\n",
      "\n",
      "loss weights                    = [ 0.33, 14.49, 19.75, 21.19, 35.05, 15.81, 21.82]\n",
      "eval loss weights (theoretical) = [ 0.33, 17.63, 15.97, 18.62, 38.64, 22.69, 17.03]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Input Error: Only 4D input Tensors are supported (got 3D)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m set_random_seed(\u001b[38;5;241m43\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m cv_models_with_scalers, cv_folds_metrics, best_models_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSlidingAttentionClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences_ds_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences_ds_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_saving_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspecies_classifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## Metrics ##\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\torch_generic_model_training.py:141\u001b[0m, in \u001b[0;36mtrain_model_with_cv\u001b[1;34m(model_provider, training_hyper_parameters, train_dataset, test_dataset, n_folds, device, save_models, model_saving_name)\u001b[0m\n\u001b[0;32m    137\u001b[0m     cv_folds_metrics\u001b[38;5;241m.\u001b[39mappend(training_run_metrics)\n\u001b[0;32m    138\u001b[0m     best_models_metrics\u001b[38;5;241m.\u001b[39mappend((best_model_train_metrics, best_model_eval_metrics, best_model_test_metrics))\n\u001b[1;32m--> 141\u001b[0m \u001b[43mtrain_with_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models_with_scalers, cv_folds_metrics, best_models_metrics\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\cross_validation_training.py:66\u001b[0m, in \u001b[0;36mtrain_with_cv\u001b[1;34m(dataset, train_func, n_folds)\u001b[0m\n\u001b[0;32m     59\u001b[0m labels_eval \u001b[38;5;241m=\u001b[39m labels_folds[eval_idx]\n\u001b[0;32m     61\u001b[0m data_train_normalized, data_validation_normalized, normalization_scaler \u001b[38;5;241m=\u001b[39m normalize_data(\n\u001b[0;32m     62\u001b[0m     data_train,\n\u001b[0;32m     63\u001b[0m     data_eval\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 66\u001b[0m \u001b[43mtrain_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_nr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNumpyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNumpyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_validation_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_eval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalization_scaler\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\torch_generic_model_training.py:50\u001b[0m, in \u001b[0;36mtrain_model_with_cv.<locals>.train_func\u001b[1;34m(fold_nr, train_ds, eval_ds, normalization_scaler)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m model \u001b[38;5;241m=\u001b[39m model_provider()\n\u001b[0;32m     49\u001b[0m latest_model, training_run_metrics, best_model, (best_model_train_metrics, best_model_eval_metrics) \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m eval_data_loader \u001b[38;5;241m=\u001b[39m create_data_loader(\n\u001b[0;32m     60\u001b[0m     eval_ds\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m     61\u001b[0m     eval_ds\u001b[38;5;241m.\u001b[39mlabels,\n\u001b[0;32m     62\u001b[0m     training_hyper_parameters\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     63\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     64\u001b[0m )\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\torch_generic_model_training.py:190\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, training_hyper_parameters, train_ds, eval_ds, device)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval loss weights (theoretical) = \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    187\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(weight, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m weight \u001b[38;5;129;01min\u001b[39;00m eval_loss_weight\u001b[38;5;241m.\u001b[39mtolist()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 190\u001b[0m training_run_metrics, best_model, best_metrics \u001b[38;5;241m=\u001b[39m \u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_hyper_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, training_run_metrics, best_model, best_metrics\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\torch_generic_model_training.py:227\u001b[0m, in \u001b[0;36m_train_model\u001b[1;34m(model, train_data_loader, eval_data_loader, optimizer, loss_weight, num_epochs, lr_scheduler, device)\u001b[0m\n\u001b[0;32m    224\u001b[0m training_run_metrics: TrainingRunMetrics \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_nr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 227\u001b[0m     train_metrics, validation_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch_nr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m         lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\torch_generic_model_training.py:272\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch_nr, model, criterion, train_data_loader, eval_data_loader, optimizer, device)\u001b[0m\n\u001b[0;32m    268\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    270\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 272\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m pred, labels \u001b[38;5;241m=\u001b[39m _reshape_for_loss(pred, labels)\n\u001b[0;32m    275\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, labels)\n",
      "File \u001b[1;32m~\\Dropbox\\jku\\2023SS\\344.091 - Machine Learning and Pattern Classification\\lib\\model\\sliding_attention_classifier.py:34\u001b[0m, in \u001b[0;36mSlidingAttentionClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m def __str__(self):\n\u001b[0;32m     28\u001b[0m     return (f'SlidingAttentionClassifier with {_count_parameters(self)} parameters, '\n\u001b[0;32m     29\u001b[0m             f'in_fnn: {_count_parameters(self.in_fnn)}, '\n\u001b[0;32m     30\u001b[0m             f'attention_stack: {_count_parameters(self.attention_stack)}, '\n\u001b[0;32m     31\u001b[0m             f'out_fnn: {_count_parameters(self.out_fnn)}')\n\u001b[0;32m     33\u001b[0m def forward(self, x: torch.Tensor) -> torch.Tensor:\n\u001b[1;32m---> 34\u001b[0m     # using batch second internally\n\u001b[0;32m     35\u001b[0m     if self.batch_first:\n\u001b[0;32m     36\u001b[0m         x = torch.swapaxes(x, 0, 1)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\modules\\fold.py:294\u001b[0m, in \u001b[0;36mUnfold.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\torch\\nn\\functional.py:4493\u001b[0m, in \u001b[0;36munfold\u001b[1;34m(input, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[0;32m   4491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mim2col(\u001b[38;5;28minput\u001b[39m, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))\n\u001b[0;32m   4492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4493\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Error: Only 4D input Tensors are supported (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Input Error: Only 4D input Tensors are supported (got 3D)"
     ]
    }
   ],
   "source": [
    "set_random_seed(43)\n",
    "\n",
    "cv_models_with_scalers, cv_folds_metrics, best_models_metrics = train_model_with_cv(\n",
    "    lambda: SlidingAttentionClassifier(ac_hyper_parameters, batch_first=True),\n",
    "    training_hyper_parameters, \n",
    "    sequences_ds_train,\n",
    "    sequences_ds_test,\n",
    "    n_folds=8,\n",
    "    device=device,\n",
    "    save_models='best',\n",
    "    model_saving_name='species_classifier',\n",
    ")\n",
    "\n",
    "\n",
    "## Metrics ##\n",
    "print(\n",
    "'''\n",
    "\n",
    "#################\n",
    "#### Metrics ####\n",
    "#################\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "cv_average_best_best_models_eval_metrics = calculate_average_metrics([m[1] for m in best_models_metrics])\n",
    "print(f'{cv_average_best_best_models_eval_metrics = } \\n\\n')\n",
    "\n",
    "if best_models_metrics[0][2] is not None:\n",
    "    cv_average_best_best_models_test_metrics = calculate_average_metrics([m[2] for m in best_models_metrics])\n",
    "    print(f'{cv_average_best_best_models_test_metrics = } \\n\\n')\n",
    "\n",
    "cv_avg_epoch_train_metrics = calculate_average_metrics_per_epoch(cv_folds_metrics)\n",
    "\n",
    "rows = []\n",
    "for fold_nr, fold_metrics in enumerate(cv_folds_metrics):\n",
    "    for epoch_nr, (train_metrics, eval_metrics) in enumerate(fold_metrics):\n",
    "        rows.append({\n",
    "            'cv_fold': fold_nr,\n",
    "            'epoch': epoch_nr,\n",
    "            'type': 'CV Train BACC',\n",
    "            'bacc': train_metrics.bacc,\n",
    "        })\n",
    "        if eval_metrics is not None:\n",
    "            rows.append({\n",
    "                'cv_fold': fold_nr,\n",
    "                'epoch': epoch_nr,\n",
    "                'type': 'CV Eval BACC',\n",
    "                'bacc': eval_metrics.bacc,\n",
    "            })\n",
    "\n",
    "cv_metrics_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrs_over_epochs(training_hyper_parameters: TrainingHyperParameters) -> list[float]:\n",
    "    lrs: list[float] = []\n",
    "    \n",
    "    lr = training_hyper_parameters.lr\n",
    "    for epoch_nr in range(training_hyper_parameters.num_epochs):\n",
    "        if epoch_nr in training_hyper_parameters.lr_scheduler_milestones:\n",
    "            lr *= training_hyper_parameters.lr_scheduler_gamma\n",
    "        lrs.append(lr)\n",
    "    return lrs\n",
    "    \n",
    "\n",
    "def plot_baccs_over_epochs(title: str):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8), dpi=300)\n",
    "    \n",
    "    cv_epoch_range = np.arange(0, training_hyper_parameters.num_epochs)\n",
    "    \n",
    "    sns.lineplot(data=cv_metrics_df, x='epoch', y='bacc', hue='type', ax=ax, palette=['blue', 'green'])\n",
    "    \n",
    "    twin_ax = ax.twinx()\n",
    "    lr_plots_kwargs = {\n",
    "        'ls': '--',\n",
    "        'lw': 0.75\n",
    "    }\n",
    "    twin_ax.plot(cv_epoch_range, create_lrs_over_epochs(training_hyper_parameters), label='CV LR', c='lightblue', **lr_plots_kwargs)\n",
    "\n",
    "    ax.set(\n",
    "        title=title,\n",
    "        xlabel='Epoch',\n",
    "        ylabel='BACC',\n",
    "        yticks=[x * 0.1 for x in range(11)]\n",
    "    )\n",
    "\n",
    "    ax.grid(ls=':')\n",
    "    ax.legend(loc='center right')\n",
    "    twin_ax.legend(loc='lower right')\n",
    "\n",
    "    for spine in ['top', 'right', 'bottom', 'left']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "        twin_ax.spines[spine].set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "average_last_epoch_eval_bacc = cv_metrics_df.query('type == \"CV Eval BACC\"').groupby('epoch').mean().iloc[-1]['bacc']\n",
    "plot_baccs_over_epochs(f'Attention Classifier, last epoch eval bacc = {average_last_epoch_eval_bacc:6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
